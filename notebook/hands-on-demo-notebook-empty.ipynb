{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Demo-notebook (SHOGUN Hands-On at ResearchGate, 2014-07-27)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook has been developed interactively at the SHOGUN Hands-On at ResearchGate, 2014-07-28.  It's meant to demonstrate how to solve a simple prediction task using SHOGUN.  It's using different tools to preprocess the data, but the learning and evaluation happens in SHOGUN.\n",
      "\n",
      "We collected 210 abstracts written in english language.  The abstracts have been published on universities of different countries: Russia, India, Germany, China.  Only use the abstract to predict the native language of the author.\n",
      "\n",
      "The data is contained in this repository."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data: English short texts from non-native authors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Task: Given english text. Predict native language of its author."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Reading RAW abstracts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "abstract_data_dir=\"../data/non-native-abstracts/\"\n",
      "\n",
      "ids = []\n",
      "abstracts = []\n",
      "\n",
      "for curr_dir, subdirs, files in os.walk(abstract_data_dir):\n",
      "    for curr_file in files:\n",
      "        ids.append(os.path.basename(curr_dir) + os.sep + curr_file)\n",
      "\n",
      "        abstract_fd = open(curr_dir + os.sep + curr_file)\n",
      "        abstracts.append(abstract_fd.read())\n",
      "\n",
      "print \"read {} abstracts from directory {} \".format(len(abstracts), abstract_data_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstracts[0]\n",
      "# abstract_pos = pos_tag_abstract(abstract_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "POS tagging using NLTK"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "# nltk.download()\n",
      "\n",
      "l_normalize_sentence = lambda t: t.strip()\n",
      "l_filter_valid_sentence  = lambda s: len(s)\n",
      "l_pos_tagger = lambda s: nltk.pos_tag(nltk.word_tokenize(s))\n",
      "\n",
      "def pos_tag_abstract(abstract) :\n",
      "    sentences = filter(l_filter_valid_sentence, map(l_normalize_sentence, abstract.replace(\"\\n\", \" \").split(\".\")))\n",
      "    sentences_pos = map(l_pos_tagger, sentences)\n",
      "    return sentences_pos\n",
      "\n",
      "abstracts_tagged = map(lambda a: pos_tag_abstract(a), abstracts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Extracting features: POS bigrams"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_dict = {}\n",
      "next_feat_index = 1\n",
      "\n",
      "labels = []\n",
      "features = []\n",
      "\n",
      "def translate_feature_names(f):\n",
      "    if f not in features_dict:\n",
      "        features_dict[f] = len(features_dict)+1\n",
      "    return features_dict[f]\n",
      "\n",
      "def extract_features(first, second) :\n",
      "    return [\n",
      "         first[1] + \"-\" + second[1]\n",
      "         # , first[1] + \":\" + first[0].lower() + \"-\" + second[1]\n",
      "         # , first[1] + \"-\" + second[1] + \":\" + second[0].lower()\n",
      "         ]\n",
      "\n",
      "for abstract_id, abstract_pos in zip(ids, abstracts_tagged) :\n",
      "    x_features_bow = []\n",
      "    for tagged_sentence in abstract_pos :\n",
      "        for first, second in nltk.bigrams(tagged_sentence) :\n",
      "            x_features_bow += extract_features(first, second)\n",
      "    \n",
      "    x_features = {}\n",
      "    for fi in map(lambda fn: translate_feature_names(fn), x_features_bow) :\n",
      "        x_features[fi] = 1\n",
      "\n",
      "    labels.append(+1 if abstract_id.startswith(\"russia/\") else -1)\n",
      "    features.append((abstract_id, x_features))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"num examples: {}, num dimensions: {}\".format(len(features), len(features_dict))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Transforming features into numpy vectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "\n",
      "def map_dict_to_dense_array(f_dict, dim):\n",
      "    a = numpy.zeros(dim)\n",
      "    for f, c in f_dict.iteritems():\n",
      "        a[f] = c\n",
      "    return a\n",
      "\n",
      "featurespace_dim = max(features_dict.values()) + 1\n",
      "features_dense = map(lambda f: map_dict_to_dense_array(f[1], featurespace_dim), features)\n",
      "\n",
      "np_features_dense = numpy.array(features_dense, dtype=numpy.float64).T\n",
      "np_features_dense.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Permuting and splitting data into train/test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# permute data\n",
      "perm_inds=numpy.random.permutation(210)\n",
      "num_train=150\n",
      "\n",
      "np_features_dense=np_features_dense[:,perm_inds]\n",
      "labels=numpy.array(labels)[perm_inds]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split data into test and train parts\n",
      "feats_train = np_features_dense[:,:num_train]\n",
      "labels_train = labels[:num_train]\n",
      "feats_test = np_features_dense[:,num_train:]\n",
      "labels_test = labels[num_train:]\n",
      "\n",
      "feats_all = np_features_dense\n",
      "labels_all = labels\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "First experiment: Train linear SVM and evaluate using ROC"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import RealFeatures, BinaryLabels, LibLinear, ROCEvaluation\n",
      "from numpy import random, mean\n",
      "\n",
      "X_train = RealFeatures(feats_train)\n",
      "Y_train = BinaryLabels(labels_train)\n",
      "X_test = RealFeatures(feats_test)\n",
      "Y_test = BinaryLabels(labels_test)\n",
      "\n",
      "svm = LibLinear(1.0, X_train, Y_train)\n",
      "svm.train()\n",
      "\n",
      "Y_pred_train = svm.apply_binary(X_train)\n",
      "Y_pred_test = svm.apply_binary(X_test)\n",
      "\n",
      "metric=ROCEvaluation()\n",
      "\n",
      "training_error=metric.evaluate(Y_train, Y_pred_train)\n",
      "test_error=metric.evaluate(Y_test, Y_pred_test)\n",
      "\n",
      "print \"training error: {:0.2f}\".format(training_error)\n",
      "print \"test error: {:0.2f}\".format(test_error)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Second experiment: Train linear SVM with stratified cross validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import *\n",
      "\n",
      "X_all = RealFeatures(feats_all)\n",
      "Y_all = BinaryLabels(labels_all)\n",
      "\n",
      "svm = LibLinear(1.0, X_all, Y_all)\n",
      "\n",
      "split = StratifiedCrossValidationSplitting(Y_all, 5)\n",
      "metric = ROCEvaluation()\n",
      "\n",
      "cross = CrossValidation(svm, X_all, Y_all, split, metric)\n",
      "cross.set_num_runs(5)\n",
      "\n",
      "result = cross.evaluate()\n",
      "result = CrossValidationResult.obtain_from_generic(result)\n",
      "\n",
      "print \"mean ROC value: {}\".format(result.mean)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Third experiment: Train linear SVM with parameter search"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Cs = 2.**numpy.linspace(-15, 16, 31)\n",
      "results = numpy.zeros(len(Cs))\n",
      "\n",
      "split = StratifiedCrossValidationSplitting(Y_all, 5)\n",
      "metric = ROCEvaluation()\n",
      "\n",
      "cross = CrossValidation(svm, X_all, Y_all, split, metric)\n",
      "cross.set_num_runs(5)\n",
      "\n",
      "for i,C in enumerate(Cs):\n",
      "    svm.set_C(C, C)\n",
      "\n",
      "    result = cross.evaluate()\n",
      "    results[i] = CrossValidationResult.obtain_from_generic(result).mean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Plotting results of parameter search"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(numpy.log2(Cs),results)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}